{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Duplicate Reviews Via Semantic Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough we will use vector embeddings to find duplicate or similar items in a movie review dataset.\n",
    "The same approach can be used to group similar photos in your photo collection, automatically categorize data, etc.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Get the data\n",
    "2. Setup Lantern (on top of self-hosted postgres, or in Lantern Cloud)\n",
    "3. Upload the data into Lantern\n",
    "4. Generate embeddings (__automated in Lantern Cloud!__)\n",
    "5. Create a vector index (__40x faster in Lantern Cloud!__)\n",
    "6. Query the database to find similar reviews\n",
    "    1. Brute Force - no vector index (takes ~1.5 hour)\n",
    "    2. Vector Index + Code (takes ~20 minutes)\n",
    "    3.  __Vector Index + SQL JOIN (takes ~40 seconds!)__\n",
    "    \n",
    "7. Bonus! Evaluate the quality of our approximate vector index\n",
    "8. Bonus! Flag Identical Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install psycopg2 datasets sentence_transformers tqdm ipywidgets pandas > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use imdb movie review dataset from [huggingface](https://huggingface.co/datasets/imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "data = load_dataset(\"imdb\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Setup Lantern\n",
    "You will need access to a Lantern database to follow through this tutorial. \n",
    "\n",
    "You can get one with 3 clicks at [Lantern Cloud](https://lantern.dev), or can set up Lantern on your own environment ([docs](https://docs.lantern.dev/get-started/install-from-binaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "LANTERN_URL=\"PUT YOUR LANTERN URL HERE\"\n",
    "if not LANTERN_URL.startswith(\"postgres:\"):\n",
    "    LANTERN_URL=input(\"Please enter your Lantern URL:\")\n",
    "# Change the database URL to yours\n",
    "def connect_db():\n",
    "    return psycopg2.connect(LANTERN_URL)\n",
    "global_conn = connect_db()\n",
    "# idle_in_transaction_session_timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload the data into Lantern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table for our movie review dataset with the following schema:\n",
    "```sql\n",
    "CREATE TABLE imdb_reviews (\n",
    "    id SERIAL PRIMARY KEY, \n",
    "    imdb_id int NOT NULL UNIQUE, \n",
    "    review text, \n",
    "    positive_review bool)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the data to our database so we can start running queries against it.\n",
    "Note that we are using [`psycopg2.extras.execute_values`](https://www.psycopg.org/docs/extras.html#psycopg2.extras.execute_values) to handle batch uploading for us behind the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_conn.autocommit = True\n",
    "with global_conn.cursor() as cur:\n",
    "    # set idle_in_transaction_session_timeout to 200ms to avoid excessive locking\n",
    "    cur.execute(\"SET idle_in_transaction_session_timeout = 200\")\n",
    "    global_conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table\n",
    "def setup_table():\n",
    "    with global_conn.cursor() as cur:\n",
    "        #cur.execute(\"abort;CREATE EXTENSION IF NOT EXISTS lantern\")\n",
    "        #cur.execute(\"DROP TABLE IF EXISTS imdb_reviews\")\n",
    "        cur.execute(\"\"\"\n",
    "CREATE TABLE imdb_reviews (\n",
    "  id SERIAL PRIMARY KEY,\n",
    "  imdb_id int NOT NULL UNIQUE,\n",
    "  review text,\n",
    "  positive_review bool\n",
    ");\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_values(conn, values, batch_size=400, logging = True):\n",
    "    start = time.time()\n",
    "    with conn.cursor() as cur:\n",
    "        batch_review = values[\"text\"]\n",
    "        batch_sentiment = values[\"label\"]\n",
    "        id_range = range(len(values[\"text\"]))\n",
    "        \n",
    "        batch = list(zip(id_range,batch_review, batch_sentiment))\n",
    "        batch = [(e[0], e[1], e[2] == 1) for e in batch]\n",
    "        psycopg2.extras.execute_values(cur, f\"INSERT INTO imdb_reviews (imdb_id,review, positive_review) VALUES %s;\", batch,\n",
    "                                      template=None, page_size=batch_size)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_table()\n",
    "insert_values(global_conn, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is just a sanity check that imdb_ids in the postgres table correspond to our array indexes in this notebook\n",
    "with global_conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT * from imdb_reviews where imdb_id = 1;\")\n",
    "    print(cur.fetchall()[0][2] == data[\"text\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate embeddings\n",
    "At this point we have all our data in our Lantern database. \n",
    "We can now go see some summary of our table in the Lantern dashboard.\n",
    "\n",
    "More importantly, we can generate embeddings through various models, add them as additional columns to our table, and create vector indexes on them through the dashboard.\n",
    "Lantern runs these operations on dedicated, workload-optimized servers, avoiding the extra load on the database instance.\n",
    "This makes sure that your database will be available with its full capacity to answer your production queries, while very compute-heavy operations are carried out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img alt=\"Generate embeddings in Lantern Cloud\" src=\"../static/generate-embeddings-dashboard.gif\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once embedding generation and index creation finish successfully, we can see the additional columns on our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with global_conn.cursor() as cur:\n",
    "    form = \"{:>26}\" * 3\n",
    "    cur.execute(\"SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE table_name = 'imdb_reviews';\")\n",
    "    print(form.format(\"column_name\",\"data_type\",\"is_nullable\"))\n",
    "    print()\n",
    "    for r in cur.fetchall():\n",
    "        # .join([\"%s\",\"%s\",\"%s\"])\n",
    "        print(form.format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider 3 approaches for solving the problem\n",
    "1. No index, full scan of the table\n",
    "2. Lantern index + python loop to aggregate results\n",
    "3. Single JOIN query to get our answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a vector index\n",
    "\n",
    "We can again use the Lantern dashboard to create a vector index on the embedding column Lantern created for us.\n",
    "Note that to create the vector index we could use the more familiar `CREATE INDEX` statement as below:\n",
    "```python\n",
    "with global_conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE INDEX lantern_demo_idx ON lantern_demo \n",
    "    USING hnsw(vec dist_cos_ops) \n",
    "    WITH (m=32, ef_construction=128, dim=384, ef=64)\"\"\")\n",
    "```\n",
    "\n",
    "But vector index creation is an expensive operation - doing it inside the database will \n",
    "    - Take longer \n",
    "    - slow down database queries for the duration of index generation\n",
    "\n",
    "Index creation done in Lantern Dashboard happens on a separate dedicated server. The resulting index is then copied over into our database and tied to postgres, as if it was created via `CREATE INDEX`.\n",
    "This saves time and database resources! It also allows for faster iteration and index parameter tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (A. and B.) Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is necessary for approaches (1) and (2) only, since embedding querying happens in python\n",
    "all_embeds = None\n",
    "with global_conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT imdb_id, review_embedding from imdb_reviews;\")\n",
    "    all_embeds = cur.fetchall() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_similar_foreach(all_embeds, use_index=True):\n",
    "    THRESHHOLD=0.07\n",
    "    # Load the next row from the dataset\n",
    "    dist_calculation_format = \"%s <-> review_embedding\"\n",
    "    if not use_index:\n",
    "        dist_calculation_format = \"cos_dist(%s, review_embedding)\"\n",
    "\n",
    "    with global_conn.cursor() as cur:\n",
    "\n",
    "        for imdb_id, embedding in tqdm(all_embeds):\n",
    "                \n",
    "            cur.execute(f\"SELECT cos_dist(%s, review_embedding) as dist, imdb_id from imdb_reviews order by {dist_calculation_format} limit 2;\", \n",
    "                        (embedding,embedding))\n",
    "            res = cur.fetchall()\n",
    "\n",
    "\n",
    "            for r in res:\n",
    "                dist, found_id = r\n",
    "                if found_id == imdb_id:\n",
    "                    continue\n",
    "                if dist < THRESHHOLD:\n",
    "                    print(f\"found similar! (distance={dist})\")\n",
    "                    query_txt = data[\"text\"][imdb_id]\n",
    "                    print(f\"Query({imdb_id}): {query_txt}\")\n",
    "                    found_txt = data[\"text\"][found_id]\n",
    "                    print(f\"Found({found_id}): {found_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 6.A: Do not use the vector index: (WIll take ~ 1.5 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "find_similar_foreach(all_embeds, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 6.B: Use the index but query it from python for each row: (WIll take ~ 25 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "find_similar_foreach(all_embeds, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 6.C: Vector Index + SQL JOIN (35seconds - 40x faster than above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitation of the above approach is that we are iterating over all movie reviews and issuing vector search operations. We can instead describe the full query to our database and have it return the final result - a list of review IDs and corresponding closest N review ids.\n",
    "\n",
    "The query in the block below does exactly that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = None\n",
    "with global_conn.cursor() as cur:\n",
    "    # increase work memory to make sure postgres has enough space to pin the relevant\n",
    "    # table entries in memory\n",
    "    cur.execute(\"SET work_mem='100MB'\")\n",
    "    # execute the actual clustering query!\n",
    "    cur.execute(\"\"\"\n",
    "SELECT\n",
    "  forall.imdb_id, \n",
    "  nearest_per_id.near_imdb_ids, nearest_per_id.imdb_dists\n",
    "FROM\n",
    "  (\n",
    "    SELECT\n",
    "      imdb_id, review_embedding\n",
    "    FROM\n",
    "      imdb_reviews\n",
    "    LIMIT 25000\n",
    "  ) AS forall\n",
    "  JOIN LATERAL (\n",
    "    SELECT\n",
    "      ARRAY_AGG(imdb_id) AS near_imdb_ids, \n",
    "      ARRAY_AGG(imdb_dist) AS imdb_dists\n",
    "    FROM\n",
    "      (\n",
    "        SELECT\n",
    "          t2.imdb_id,\n",
    "          forall.review_embedding <=> t2.review_embedding AS imdb_dist\n",
    "        FROM\n",
    "          imdb_reviews t2\n",
    "        ORDER BY\n",
    "          imdb_dist\n",
    "        LIMIT\n",
    "          5\n",
    "      ) AS __unused_name\n",
    "  ) nearest_per_id ON TRUE\n",
    "ORDER BY\n",
    "  forall.imdb_id;\n",
    "\"\"\")\n",
    "    final_res = cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What's going on in that query?__\n",
    "\n",
    "There are two main subuqeries in the query above\n",
    "\n",
    "1. Subquery forall:\n",
    "This is the first building block of the query. It selects two pieces of information for each review in the dataset: the unique movie identifier (imdb_id) and the 'review embedding' (review_embedding). The review_embedding is a numerical representation of the review's content. This subquery is limited to the first 100,000 entries in the imdb_reviews_new1 table, indicating a focus on a specific portion of the dataset.\n",
    "\n",
    "\n",
    "2. Lateral Join Subquery nearest_per_id:\n",
    "The second building block is a more complex subquery that performs a lateral join. This means it takes each row from the forall subquery and finds the top 5 closest reviews to it based on the cosine distance between their embeddings. The cosine distance is a measure used to determine how similar two documents are in the context of natural language processing. This subquery aggregates the IDs (imdb_id) and distances (imdb_dist) of these closest reviews into arrays, essentially creating a list of most similar reviews for each review in the forall subset.\n",
    "\n",
    "__Relation of the Outer Query to Building Block Queries:__\n",
    "The outer query brings together these building blocks. It selects the imdb_id from the forall subquery and pairs it with the arrays of nearest imdb_ids and their corresponding distances (imdb_dists) from the nearest_per_id subquery. By joining these components, the query effectively maps each movie in the original subset to a list of movies with the most similar reviews, along with the degree of similarity. \n",
    "The final output is ordered by the imdb_id from the forall subset, providing an organized list of movies and their closest counterparts in terms of review content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_similar = [r for r in final_res if 0.01 < r[2][1] and r[2][1] <= 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(very_similar, columns=[\"imdb_id\", \"most_similar_imdb_ids\", \"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag similar reviews\n",
    "#### Below are some example pairs of reviews marked as similar according to our filtering above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"text\"][14384], \"\\n__VS__\\n\",data[\"text\"][14396])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data[\"text\"][16401], \"\\n__VS__\\n\",data[\"text\"][16408])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus! Evaluate the quality of our approximate vector index\n",
    "Since we know for sure each vector must be closest to itself, we can use the clustering results to see how well our approximate index keeps this invariant. An exact vector index would always keep this invariant. HNSW sacrifices exactness for performance and it gives us 3 hyper-parameters to tune how close it tries to get to the exact index. Obviously, the more exact we make our approximate index, the slower it will become, so there is a tradeoff here.\n",
    "\n",
    "We can again use Lantern dashboard to create indexes with different parameters and see which one results in fewer mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = [r for r in final_res if r[0] not in r[1]]\n",
    "print(\"In %d (of %d reviews) the review in query was not considered close to itself by our index \" % (len(mistakes), len(final_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = [r for r in final_res if r[0] not in r[1]]\n",
    "print(\"In %d (of %d reviews) the review in query was not considered close to itself by our index \" % (len(mistakes), len(final_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus! Flag Identical Reviews\n",
    "We can also use the query results from our index to find identical duplicate reviews in the dataset. To do this, we will search for vectors that are extremely close together and have different IMDB ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identical = [r for r in final_res if r[2][1] < 0.01]\n",
    "identical[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that there are many pairs of reviews that have very close to zero distance, This gives us very high confidence that the underlying reivews are identical Below are example identical rows, taken from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"][357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"][10274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
