{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyenv virtualenv 3.8.2 myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mwxml\n",
    "!pip install tqdm\n",
    "!pip install mwxml psycopg2-binary\n",
    "!pip install ipywidgets\n",
    "!pip install groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = \"postgresql://postgres:postgres@localhost:6666\"\n",
    "import mwxml\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "\n",
    "# Database connection URL\n",
    "db_url = conn_string\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(db_url)\n",
    "def SQL(query, conn_string = conn_string):\n",
    "    with psycopg2.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            try:\n",
    "                res = cursor.fetchall()\n",
    "                return res\n",
    "            except Exception as e:\n",
    "                if str(e) == 'no results to fetch':\n",
    "                    return None\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Database connection URL\n",
    "db_url = conn_string\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(db_url)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table if not exists\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS wikipedia_pages;\n",
    "CREATE TABLE IF NOT EXISTS wikipedia_pages (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    page_title TEXT,\n",
    "    revision_id BIGINT,\n",
    "    timestamp TIMESTAMP,\n",
    "    contributor TEXT,\n",
    "    text TEXT\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "def insert_pages(pages):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO wikipedia_pages (page_title, revision_id, timestamp, contributor, text)\n",
    "    VALUES (%s, %s, %s, %s, %s);\n",
    "    \"\"\"\n",
    "    execute_batch(cursor, insert_query, pages)\n",
    "    conn.commit()\n",
    "\n",
    "# Load and parse the Wikipedia dump\n",
    "batch_size = 1000\n",
    "pages = []\n",
    "\n",
    "dump_path = \"hywiki-20240701-pages-articles.xml\"\n",
    "\n",
    "with open(dump_path, 'rb') as f:\n",
    "    dump = mwxml.Dump.from_file(f)\n",
    "    for page in tqdm(dump):\n",
    "        for revision in page:\n",
    "            pages.append(\n",
    "                (\n",
    "                    page.title,\n",
    "                    revision.id,\n",
    "                    datetime.fromisoformat(str(revision.timestamp)),\n",
    "                    revision.contributor.user_text if hasattr(revision, 'contributor') else None,\n",
    "                    revision.text\n",
    "                )\n",
    "            )\n",
    "            if len(pages) >= batch_size:\n",
    "                insert_pages(pages)\n",
    "                pages = []\n",
    "\n",
    "    if pages:\n",
    "        insert_pages(pages)\n",
    "\n",
    "# Close the database connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a connection via a context and run this\n",
    "\n",
    "add_tsvector_column('wikipedia_pages', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a query retrieving all the rows related to armenian mathematicians\n",
    "\n",
    "with psycopg2.connect(conn_string) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "SELECT page_title FROM wikipedia_pages\n",
    "WHERE text_simple ilike  '%մաթեմատիկակա%' LIMIT 100;\n",
    "\"\"\")\n",
    "        for row in cursor:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SQL(\"\"\"\n",
    "    SELECT (string_to_array(lower(regexp_replace(text_simple, '\\\\W+', ' ', 'g')), ' ')) AS word, text_simple\n",
    "    FROM wikipedia_pages LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"SELECT page_title, text from wikipedia_pages where id = 79 LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "    \n",
    "    SELECT\n",
    "    tf.id,\n",
    "    tf.word,\n",
    "    tf.page_title,\n",
    "    tf.frequency,\n",
    "    tw.total,\n",
    "    round(tf.frequency::decimal / tw.total, 2)::float AS term_frequency\n",
    "FROM\n",
    "    term_frequency tf\n",
    "JOIN\n",
    "    total_words tw ON tf.id = tw.id\n",
    "ORDER BY\n",
    "    tf.frequency::decimal / tw.total DESC, tf.id, tf.word;\n",
    "    \n",
    "    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(f\"\"\"\n",
    "DROP TABLE IF EXISTS unique_terms;\n",
    "CREATE TABLE unique_terms AS\n",
    "SELECT\n",
    "    id,\n",
    "    UNNEST(string_to_array(lower(regexp_replace(text_simple, '\\\\W+', ' ', 'g')), ' ')) AS word\n",
    "FROM\n",
    "    (SELECT * FROM wikipedia_pages LIMIT {subset_size}) a\n",
    "GROUP BY\n",
    "    id, word;\n",
    "--SELECT word, count(word) from unique_terms group by word order by count(word) desc;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "-- calculated in how many documents the given unique term appears in\n",
    "DROP TABLE IF EXISTS doc_frequency;\n",
    "CREATE TABLE doc_frequency AS\n",
    "SELECT\n",
    "    word,\n",
    "    COUNT(DISTINCT id) AS doc_count\n",
    "FROM\n",
    "    unique_terms\n",
    "GROUP BY\n",
    "    word;\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# -- Calculate the total number of documents\n",
    "# WITH total_docs AS (\n",
    "#     SELECT COUNT(*) AS total FROM wikipedia_pages\n",
    "# )\n",
    "\n",
    "# -- Calculate the number of documents containing each term\n",
    "# ,\n",
    "# -- Calculate the IDF for each term\n",
    "# idfs AS (\n",
    "#     SELECT\n",
    "#         word,\n",
    "#         LOG((SELECT total FROM total_docs)::decimal / df.doc_count) AS idf\n",
    "#     FROM\n",
    "#         doc_frequency df\n",
    "\n",
    "# )\n",
    "# SELECT * from idfs order by idf desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires CREATE EXTENSION fuzzystrmatch;\n",
    "SQL(\"\"\"\n",
    "    SELECT word, count(*) from unique_terms where levenshtein('աստղակերպերի', word) < 2 group by word;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "with res as (\n",
    "\n",
    "    SELECT\n",
    "        (array_agg(corpus.page_title))[1],\n",
    "        array_agg(tf.word),\n",
    "        array_agg(tf.frequency),\n",
    "        array_agg(df.doc_count),\n",
    "        array_agg(cardinality(string_to_array(lower(regexp_replace(text_simple, '\\\\W+', ' ', 'g')), ' '))),\n",
    "        SUM(tf.frequency::decimal / cardinality(string_to_array(lower(regexp_replace(text_simple, '\\\\W+', ' ', 'g')), ' '))\n",
    "        * LOG((SELECT COUNT(*) AS count FROM wikipedia_pages) / df.doc_count)) AS tf_idf\n",
    "        \n",
    "        \n",
    "    FROM\n",
    "        (SELECT * FROM wikipedia_pages LIMIT 10000) corpus\n",
    "    JOIN term_frequency tf ON corpus.id = tf.id\n",
    "    JOIN doc_frequency df ON tf.word = df.word\n",
    "    WHERE\n",
    "        tf.word = ANY(string_to_array('պետություն մայրաքաղաք բութան տոգո', ' '))\n",
    "    GROUP BY\n",
    "        tf.id\n",
    "    ) SELECT * from res order by tf_idf desc;\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL(\"\"\"\n",
    "-- Create a function to search based on a query string using TF-IDF\n",
    "CREATE OR REPLACE FUNCTION search_documents(query TEXT)\n",
    "RETURNS TABLE(id INT, score DECIMAL) AS $$\n",
    "DECLARE\n",
    "    query_words TEXT[];\n",
    "BEGIN\n",
    "    -- Tokenize the query string into an array of words\n",
    "    query_words := string_to_array(query, ' ');\n",
    "\n",
    "    RETURN QUERY\n",
    "    SELECT\n",
    "        tf.id,\n",
    "        SUM(tf.tf_idf) AS score\n",
    "    FROM\n",
    "        tf_idf tf\n",
    "    WHERE\n",
    "        tf.word = ANY(query_words)\n",
    "    GROUP BY\n",
    "        tf.id\n",
    "    ORDER BY\n",
    "        score DESC;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almost Unique terms\n",
    "\n",
    "There are too many unique terms as calculated above\n",
    "~13M for the 100_000 subset of the wiki data. The problem is that lammetization does not work for armenian in postgres. Luckily, we can use levenshtein distance to consider words with small edit distance the same!\n",
    "\n",
    "The problem is that it is an O(N^2) algorithm to find almost unique words from the unique_words table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH word_distances AS (\n",
    "    SELECT\n",
    "        w1.id AS id1,\n",
    "        w1.word AS word1,\n",
    "        w2.id AS id2,\n",
    "        w2.word AS word2,\n",
    "        levenshtein(w1.word, w2.word) AS distance\n",
    "    FROM\n",
    "        (SELECT * FROM unique_terms LIMIT 10000) w1,\n",
    "        (SELECT * FROM unique_terms LIMIT 10000) w2\n",
    "    WHERE\n",
    "        w1.id <> w2.id\n",
    "),\n",
    "unique_words AS (\n",
    "    SELECT\n",
    "        id1 AS id,\n",
    "        word1 AS word\n",
    "    FROM\n",
    "        word_distances\n",
    "    GROUP BY\n",
    "        id1, word1\n",
    "    HAVING\n",
    "        MIN(distance) >= 2\n",
    ")\n",
    "SELECT COUNT(*)\n",
    "FROM unique_words;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imporve the situation by defining hnsw vector index op classes for text data type and levenshtein distance metric!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
