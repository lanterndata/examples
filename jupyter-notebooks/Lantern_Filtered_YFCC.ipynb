{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pgpq\n",
    "!python -m pip install psycopg\n",
    "!python -m pip install SQLAlchemy\n",
    "!pip install pinecone-datasets\n",
    "# make sure to authenticate to gcp before reading parquet from there via pandas.read_parquet!!\n",
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_datasets import list_datasets, load_dataset, Dataset\n",
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg\n",
    "import lib\n",
    "from importlib import reload\n",
    "from sqlalchemy import create_engine\n",
    "from time import time\n",
    "reload(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = \"postgresql://postgres:postgres@localhost:6666\"\n",
    "lib.recreate_tables(conn_string)\n",
    "lib.create_extensions(conn_string)\n",
    "conn = psycopg.connect(conn_string)\n",
    "engine = create_engine(conn_string, echo=True)\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lib)\n",
    "yfcc_data = lib.get_yfcc_data(dataset='10M') # 100K\n",
    "yfcc_data_queries = lib.get_yfcc_data(queries=True, dataset='10M') # 100k\n",
    "yfcc_data_queries = lib.get_yfcc_data(queries=True, size='10M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map blob column to jsonb\n",
    "lib.df2pg(conn_string, yfcc_data)\n",
    "lib.df2pg(conn_string, yfcc_data_queries, queries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT id, metadata_tags, blob FROM yfcc_passages LIMIT 1\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_sql('select * from yfcc_passages limit 10', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from yfcc_passages limit 10', con=engine)\n",
    "# select only rows from the pg table that have blob->selectibity < 10\n",
    "df = pd.read_sql(\"select * from yfcc_queries where blob->>'selectivity'< \\'10\\'\", con=engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT id, embedding FROM yfcc_10m WHERE metadata @> '{\"tags\": [\"108757\"]}' ORDER BY embedding <-> '[...]' LIMIT 10;\n",
    "# run this query with engine ORDER BY vector <-> '{0.1, 0.2, 0.3}'\n",
    "# get all rows where metadata array has more than one element\n",
    "# df = pd.read_sql(\"\"\"SELECT id, metadata FROM yfcc_data WHERE array_length(metadata, 1) > 1 ORDER BY id LIMIT 100 \"\"\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srecall = 0\n",
    "nrecall = 0\n",
    "LIMIT=300\n",
    "#create a max heap\n",
    "from heapq import heapify, heappush, heappop\n",
    "heap = []\n",
    "\n",
    "heapify(heap)\n",
    "\n",
    "lib.pg_stat_reset(conn_string)\n",
    "\n",
    "for i in range(LIMIT):\n",
    "    # measure the time the next line\n",
    "    t = time()\n",
    "    r, recall =lib.vector_search(conn_string,  q_vector_id=i, explain = False, materialize_first=True, return_recall=True, reuse_conn=True)\n",
    "    search_time = time()-t\n",
    "    heappush(heap, (-search_time*1000, i))\n",
    "    if len(heap) > 10:\n",
    "        heappop(heap)\n",
    "    srecall += recall\n",
    "    nrecall += 1\n",
    "print(\"index stats:\", lib.pg_stat_show(conn_string))\n",
    "print(\"recall is\", srecall/nrecall)\n",
    "print(\"slowest times are (ms, id)\", heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lib)\n",
    "# lib.vector_search(conn_string, [3432], explain = False)\n",
    "# lib.create_index(conn_string)\n",
    "res = lib.bulk_vector_search(conn_string, 3000,k=10, return_recall=True, explain=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if recall is none, means near_ids and near_dists are none, means no results were found\n",
    "overall_recall = sum([1 if a.recall is None else a.recall for a in res])/len(res)\n",
    "tiebreak_affected = [a for a in res if a.recall and int(a.recall) < 1  ]\n",
    "# note: even with 100% accurate scan, recall is < 1, since there are 4 rows which have equal distance to 10th and 11th result\n",
    "# and there is no stable tie breaking\n",
    "print(overall_recall, len(tiebreak_affected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select * from yfcc_queries where (blob->'selectivity')::integer > 10 \", con=engine)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
