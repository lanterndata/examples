{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 2 was recently released and not all packages support it yet.\n",
    "# of course they would change the latest default and not be backward compatible...\n",
    "!pip uninstall -y numpy\n",
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!python -m pip install pgpq\n",
    "!python -m pip install psycopg\n",
    "!python -m pip install SQLAlchemy\n",
    "!pip install pinecone-datasets\n",
    "!pip install psycopg2-binary\n",
    "!pip install matplotlib\n",
    "!pip install plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_datasets import list_datasets\n",
    "list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg\n",
    "import lib\n",
    "from importlib import reload\n",
    "from sqlalchemy import create_engine\n",
    "reload(lib)\n",
    "\n",
    "# for parallel index creation\n",
    "from multiprocessing import Pool \n",
    "\n",
    "# for plotting\n",
    "from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap\n",
    "from plotnine import *\n",
    "from plotnine.data import mtcars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = \"postgresql://postgres:postgres@localhost:4444\"\n",
    "lib.recreate_tables(conn_string)\n",
    "lib.create_extensions(conn_string)\n",
    "conn = psycopg.connect(conn_string)\n",
    "engine = create_engine(conn_string, echo=True)\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = '10M'\n",
    "DATASET = '100K'\n",
    "\n",
    "yfcc_data = lib.get_yfcc_data(dataset=DATASET)\n",
    "yfcc_data_queries = lib.get_yfcc_data(queries=True, dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map blob column to jsonb\n",
    "lib.df2pg(conn_string, yfcc_data)\n",
    "# if the DB instance is small, tthe above may fail with OOM at _transform_metadata step. In that case you can specifically rerun that step\n",
    "# lib._transform_metadata(conn_string)\n",
    "lib.df2pg(conn_string, yfcc_data_queries, queries=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploreatory queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT id, metadata_tags, blob FROM yfcc_passages LIMIT 1\")\n",
    "    print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from yfcc_passages limit 10', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('select * from yfcc_passages limit 10', con=engine)\n",
    "# select only rows from the pg table that have blob->selectibity < 10\n",
    "df = pd.read_sql(\"select blob->>'selectivity', * from yfcc_queries where blob->>'selectivity'> \\'10\\' limit 10\", con=engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bring vector values to 0-1 range for quantization\n",
    "with psycopg.connect(conn_string) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "UPDATE yfcc_passages SET vector =  (\n",
    "  SELECT array_agg((element - 128)/ 100.0)\n",
    "  FROM unnest(vector) AS t(element)\n",
    ");\n",
    "UPDATE yfcc_queries SET vector =  (\n",
    "  SELECT array_agg((element - 128)/ 100.0)\n",
    "  FROM unnest(vector) AS t(element)\n",
    ");        \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create partial indexes\n",
    "\n",
    "with psycopg.connect(conn_string) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "    DROP FUNCTION IF EXISTS create_index_statements_for_popular_tags(index_threshhold INTEGER);\n",
    "    CREATE OR REPLACE FUNCTION create_index_statements_for_popular_tags(index_threshhold INTEGER DEFAULT 10000)\n",
    "    RETURNS TABLE(index_command TEXT) AS\n",
    "    $$\n",
    "    DECLARE\n",
    "        tag_record RECORD;\n",
    "    BEGIN\n",
    "        FOR tag_record IN\n",
    "            SELECT tag\n",
    "            FROM (\n",
    "                SELECT unnest(metadata_tags) AS tag\n",
    "                FROM yfcc_passages\n",
    "            ) AS tags\n",
    "            GROUP BY tag\n",
    "            HAVING COUNT(*) > index_threshhold\n",
    "            ORDER BY COUNT(*) DESC\n",
    "        LOOP\n",
    "            index_command := format('CREATE INDEX IF NOT EXISTS hnsw_filtered_%s ON yfcc_passages USING lantern_hnsw(vector) WITH (quant_bits = 8) WHERE metadata_tags @> ARRAY[%s];', tag_record.tag, tag_record.tag);\n",
    "            RETURN NEXT;\n",
    "        END LOOP;\n",
    "    END;\n",
    "    $$ LANGUAGE plpgsql;\n",
    "                    \"\"\")\n",
    "    # for s in create_index_statements:\n",
    "    #     print(\"running\", s)\n",
    "    #     cursor.execute(s[0])\n",
    "# done in 792m 11.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(conn_string) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        create_index_statements = cursor.execute(\"select * from create_index_statements_for_popular_tags();\").fetchall()\n",
    "create_index_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES=10\n",
    "print(\"creating\", len(create_index_statements), \"partial indexes\")\n",
    "reload(lib)\n",
    "            \n",
    "            \n",
    "# python create paiers of (conn_string, index_command) and run them in parallel\n",
    "run_query_inputs = [(conn_string, s[0]) for s in create_index_statements]\n",
    "\n",
    "with Pool(NUM_CORES) as p:\n",
    "    p.starmap(lib.run_query, run_query_inputs, chunksize=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prewarm everything\n",
    "\n",
    "with psycopg.connect(conn_string) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        print(\"prewarm the base table\")\n",
    "        # prewarming the base relation first, since this has lowest priority\n",
    "        # and it is ok if this gets evicted later to make space for the rest\n",
    "        cursor.execute(\"SELECT pg_prewarm('yfcc_passages')\")    \n",
    "        \n",
    "        print(\"prewarm all partial indexes\")\n",
    "        cursor.execute(\"\"\"\n",
    "SELECT pg_prewarm(i.relname::text)\n",
    "FROM pg_class t\n",
    "JOIN pg_index ix ON t.oid = ix.indrelid\n",
    "JOIN pg_class i ON i.oid = ix.indexrelid\n",
    "JOIN pg_am a ON i.relam = a.oid\n",
    "JOIN pg_namespace n ON n.oid = i.relnamespace\n",
    "WHERE a.amname = 'lantern_hnsw';\n",
    "                       \"\"\")\n",
    "        \n",
    "        print(\"prewarm pk and GIN indexes on yfcc_passages\")\n",
    "        cursor.execute(\"\"\"\n",
    "SELECT i.relname, pg_prewarm(i.relname::text)\n",
    "FROM pg_class t\n",
    "JOIN pg_index ix ON t.oid = ix.indrelid\n",
    "JOIN pg_class i ON i.oid = ix.indexrelid\n",
    "JOIN pg_am a ON i.relam = a.oid\n",
    "JOIN pg_namespace n ON n.oid = i.relnamespace\n",
    "WHERE (a.amname = 'gin' OR a.amname = 'btree') AND t.relname = 'yfcc_passages';\n",
    "                       \"\"\")\n",
    "\n",
    "        \n",
    "        \n",
    "        print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for use_pgvector in [True, False]:\n",
    "reload(lib)\n",
    "for use_pgvector in [False]:\n",
    "\n",
    "    recalls, latencies, stats = lib.run_experiment(conn_string, 10000, offset=0, pgvector=use_pgvector, explain=False)\n",
    "    plt.hist(latencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"recalls.json\", \"w\") as f:\n",
    "    json.dump(recalls.tolist(), f)\n",
    "    \n",
    "with open(\"latencies.json\", \"w\") as f:\n",
    "    json.dump(latencies.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files above\n",
    "with open(\"recalls.json\", \"r\") as f:\n",
    "    recalls = np.array(json.load(f))\n",
    "with open(\"latencies.json\", \"r\") as f:\n",
    "    latencies = np.array(json.load(f))\n",
    "with open(\"latencies-pinecone-100k.json\", \"r\") as f:\n",
    "    latencies_pinecone = np.array(json.load(f))\n",
    "with open(\"recalls-pinecone-100k.json\", \"r\") as f:\n",
    "    recalls_pinecone = np.array(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10_000_000\n",
    "vector_dim = 192\n",
    "M = 16\n",
    "index_size = 1.5 * dataset_size * vector_dim * M / 1024 / 1024 / 1024\n",
    "qpm = 1000\n",
    "ipm = 1000\n",
    "\n",
    "# Per pinecone's pricing of 18 RUs per query on YFCC dataset.\n",
    "\n",
    "\n",
    "qpm = [1, 10, 100, 1000, 10000]\n",
    "qp_month = 30 * 24 * 60 * np.array(qpm)\n",
    "ru_per_month = 18 * np.array(qp_month)\n",
    "cost_per_1M_ru = 8.25 # https://www.pinecone.io/pricing/\n",
    "query_cost_per_month = ru_per_month * cost_per_1M_ru / 1_000_000\n",
    "\n",
    "ec2_cost_per_month = 153 # r6g.2xlarge 64 GB 8 core shared ec2 instance, easily be able to support 10K/queries per minute\n",
    "ubicloud_cost_per_month = 520\n",
    "lantern_on_gcp = 965\n",
    "lantern_on_ubicloud = 595\n",
    "\n",
    "\n",
    "# plot the above with ggplot such that. X axis is various queries per minute query rates, Y axis is cost per month\n",
    "# each group of bars are pinecone, ec2, ubicloud, lantern on gcp and lantern on ubicloud\n",
    "\n",
    "df_cost = pd.DataFrame({\n",
    "    \"Queries per minute\": qpm,\n",
    "    \"Pinecone\": query_cost_per_month,\n",
    "    \"EC2\": ec2_cost_per_month,\n",
    "    # \"Ubicloud\": ubicloud_cost_per_month,\n",
    "    \"Lantern\": lantern_on_gcp,\n",
    "    \"Lantern on Ubicloud\": lantern_on_ubicloud\n",
    "})\n",
    "\n",
    "df_cost_melted = df_cost.melt(id_vars=\"Queries per minute\", var_name=\"System\", value_name=\"Cost per month\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cost_melted[\"aa\"] = df_cost_melted[\"Queries per minute\"]\n",
    "system_colors = [\"#FFFF00\", \"#ff7f0e\",\"#FF564B\", \"#1C17FF\"]\n",
    "system_order = [\"EC2\", \"Lantern on Ubicloud\", \"Lantern\", \"Pinecone\"]\n",
    "\n",
    "# let cc column be cost per month for pinecone only and be none for other systems\n",
    "df_cost_melted[\"cc\"] = df_cost_melted[\"Cost per month\"]\n",
    "df_cost_melted.loc[df_cost_melted[\"System\"] != \"Pinecone\", \"cc\"] = None\n",
    "\n",
    "# Create a categorical type with the specified order\n",
    "df_cost_melted['System'] = pd.Categorical(df_cost_melted['System'], categories=system_order, ordered=True)\n",
    "\n",
    "cost_plot = (\n",
    "    \n",
    "    ggplot(df_cost_melted, aes(x=\"factor(aa)\", y='Cost per month', fill='System')) +\n",
    "    geom_bar(stat='identity', position='dodge') +\n",
    "    geom_text(aes(label='map( lambda x: \"$\" + str(int(x)) if x == x else \"\", cc)'), position=position_dodge(width=1), size=9, va='bottom', color=\"#1C17FF\")+\n",
    "    labs(x='Queries per minute', y='Cost per month') +\n",
    "    scale_y_continuous(labels=lambda l: [\"$\" + str(int(x)) for x in l], breaks=range(0, 6600, 500)) +\n",
    "    # change ordering of systems\n",
    "    \n",
    "    coord_cartesian(ylim=(0, 6600)) +\n",
    "    gg_theme +\n",
    "    scale_fill_manual(values=system_colors) +\n",
    "    # x == x filters out float('NaN') values which are used above to filter out cost numbers and reduce clutter\n",
    "    ggtitle(\"Cost per month for various systems\")\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "fig = cost_plot.draw()\n",
    "fig.axes[0].collections[0].set_clip_on(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.savefig(\"cost_plot.png\", dpi=300)\n",
    "fig\n",
    "# cost_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(recalls_pinecone, 100 - np.array([50, 90, 95, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print 50, 90, 95 and 99 percentile recall and latency on the same line per percentile in a tabulated format\n",
    "print(\"percentile\\trecall PG\\t \\tlatency(ms)\")\n",
    "print(\"================================\")\n",
    "print(f\"mean\\t\\t{np.round(np.mean(recalls),2)}\\t{np.round(np.mean(latencies), 2)}\")\n",
    "for p in [50, 90, 95, 99]:\n",
    "    print(f\"{p}\\t\\t{np.percentile(recalls, 100-p)}\\t{np.round(np.percentile(latencies, p), 2)}\")\n",
    "\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = np.array([50, 95, 99])\n",
    "\n",
    "latencies_percentiles = np.percentile(latencies, percentiles)\n",
    "pinecone_latencies = np.percentile(latencies_pinecone, percentiles)\n",
    "\n",
    "postgres_recalls = np.percentile(recalls, 100 - percentiles)\n",
    "pinecone_recalls = np.percentile(recalls_pinecone, 100-percentiles)\n",
    "\n",
    "\n",
    "# Create a DataFrame for the percentiles\n",
    "df_percentiles = pd.DataFrame({'Percentile': percentiles, 'Latency': latencies_percentiles, \"Recall\":postgres_recalls, \"System\": \"Postgres\"})\n",
    "df_percentiles_pinecone = pd.DataFrame({'Percentile': percentiles, 'Latency': pinecone_latencies, \"Recall\": pinecone_recalls, \"System\": \"Pinecone\"})\n",
    "df_percentiles = pd.concat([df_percentiles, df_percentiles_pinecone])\n",
    "\n",
    "\n",
    "#common plot components\n",
    "gg_theme = theme_bw() + theme(figure_size=(6, 3))\n",
    "# gg_theme = theme_minimal() + theme(figure_size=(6, 3))\n",
    "gg_x_axis_percentiles =  scale_x_discrete(labels=lambda l: [str(x) + 'th' for x in l])\n",
    "\n",
    "theme\n",
    "\n",
    "# Plot the percentiles\n",
    "plot = (\n",
    "    ggplot(df_percentiles, aes(x='factor(Percentile)', y='Latency', fill='System')) +\n",
    "    geom_bar(stat='identity', position='dodge', width=0.7) +\n",
    "    geom_text(aes(label='map( lambda x: str(int(x)), round(Latency))'), position=position_dodge(width=0.7), size=10, va='baseline') +\n",
    "    labs(x='Percentile', y='Latency') +\n",
    "    scale_y_continuous(labels=lambda l: [str(int(x)) + \"ms\" for x in l]) +\n",
    "\n",
    "    gg_x_axis_percentiles +\n",
    "    gg_theme +\n",
    "    scale_fill_manual(values=system_colors[2:][::-1]) +\n",
    "    ggtitle(\"Latency at various percentiles\")\n",
    ")\n",
    "plot.show()\n",
    "\n",
    "\n",
    "plot_recall = (\n",
    "    ggplot(df_percentiles, aes(x='factor(Percentile)', y='Recall', fill='System')) +\n",
    "    geom_bar(stat='identity', position='dodge', width=0.7) +\n",
    "    geom_text(aes(label='map( lambda x: str(int(x)) + \"%\", round(Recall * 100))'), position=position_dodge(width=.7), size=10, va='baseline') +\n",
    "    labs(x='Percentile', y='Recall') +\n",
    "    scale_y_continuous(labels=lambda l: [str(int(x * 100)) + \"%\" for x in l], limits=(0,1)) +\n",
    "    gg_x_axis_percentiles +\n",
    "    gg_theme +\n",
    "    scale_fill_manual(values=system_colors[2:][::-1]) +\n",
    "\n",
    "    ggtitle(\"Recall at various percentiles\")\n",
    "    \n",
    "\n",
    ")\n",
    "plot_recall.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain output\n",
    "lib.vector_search(conn_string,  q_vector_id=21, explain = True, materialize_first=True, return_recall=True, reuse_conn=True, pgvector=False, prefilter_count=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lib)\n",
    "# lib.vector_search(conn_string, [3432], explain = False)\n",
    "# lib.create_index(conn_string)\n",
    "res = lib.bulk_vector_search(conn_string, 3000,k=10, return_recall=True, explain=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if recall is none, means near_ids and near_dists are none, means no results were found\n",
    "overall_recall = sum([1 if a.recall is None else a.recall for a in res])/len(res)\n",
    "tiebreak_affected = [a for a in res if a.recall and int(a.recall) < 1  ]\n",
    "# note: even with 100% accurate scan, recall is < 1, since there are 4 rows which have equal distance to 10th and 11th result\n",
    "# and there is no stable tie breaking\n",
    "print(overall_recall, len(tiebreak_affected))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
